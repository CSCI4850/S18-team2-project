{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Next-Day Bitcoin Price Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Members: Bishoy Boktor, Yonathan Feleke, Ryan Florida, Kyle Lutz, Joel Norris, Karla Robles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to yield a reasonable estimate for the price of Bitcoin, which is a virtual currency built upon block-chain technology that is gaining quite a bit of popularity, one day in advance. There was also a discussion about  the potential use of sentiment analysis (specifically from the popular social media site Twitter) in order to predict whether the price of Bitcoin would increase or decrease on the following day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific role of neural nets in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are planning to construct a recurrent neural network, a specific neural model which excels at sequential data via a series of specialized cells which allow the model to retain and forget important aspects of data, and train the network on the daily price fluctuations of Bitcoin. The data we plan to  use was collected every day over a five year span. We will then use this recurrent neural network to make a prediction about the most probable value for the price on the following day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set(s) to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are planning to use the “Cryptocurrency Historical Prices” dataset provided by Sudala Raj Kumar on Kaggle.com. If we decide to incorporate sentiment analysis, then we will also have to synthesize our own dataset of “relevant” information from popular social media sites. In this context, “relevant” will be defined by the influence that the author of a given post has (i.e. number of followers, level of notoriety, etc). To do this, we can take advantage of Twitter’s tweet-searching API and an our own Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifiability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will verify the efficacy of our network by using the standard 20/80 split on the dataset, where 20 percent of the data is used for testing and the remaining 80 percent of the data is used for training. To achieve this, we will essentially have a sliding window of training data that will predict the price of the testing data within that window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
